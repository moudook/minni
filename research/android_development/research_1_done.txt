The Strategic Reorientation of Mobile Neural Inference: A Comprehensive Analysis of the Transition from the Neural Networks API to Unified Hardware-Accelerated Runtimes in Android 15The landscape of on-device machine learning (ODML) has reached a critical inflection point, necessitated by the rapid divergence between the stagnant cycles of mobile operating system updates and the exponential acceleration of neural network architectures. Since its inception in Android 8.1 (API level 27), the Neural Networks API (NNAPI) served as the foundational abstraction layer for hardware-accelerated inference, providing a unified C API to bridge high-level frameworks like TensorFlow Lite with vendor-specific silicon. However, the emergence of transformer-based models, diffusion networks, and the broader shift toward Generative AI (GenAI) has exposed the architectural limitations of platform-integrated APIs. With the release of Android 15 (API level 35), the industry is witnessing the formal deprecation of NNAPI in favor of LiteRT, a decoupled, updatable framework designed to provide more direct access to specialized Neural Processing Units (NPUs). This transition represents a fundamental paradigm shift from a general-purpose abstraction toward highly specialized, hardware-aware execution environments.The Evolution and Deprecation of the Neural Networks APIThe Neural Networks API was originally conceived to provide a "base layer" of functionality for machine learning frameworks. By standardizing the interface between software and hardware, NNAPI allowed developers to write code once that could, in theory, run on any compatible accelerator, including Graphics Processing Units (GPUs), Digital Signal Processors (DSPs), and dedicated NPUs.Technical Iterations and Platform IntegrationThroughout its lifecycle, NNAPI underwent several significant revisions to accommodate the increasing complexity of neural workloads. Android 10 introduced support for device discovery and the use of native hardware buffers (AHardwareBuffer), enabling zero-copy data transfers between the driver and the application. Android 11 further expanded the API with support for signed 8-bit quantization, control flow operations (IF and WHILE), and Quality of Service (QoS) priorities, which allowed applications to specify the relative importance of different models.Android VersionAPI LevelKey NNAPI EnhancementsHardware Abstraction Layer (HAL)Android 8.127Initial Release; core ML operationsHIDL 1.0Android 928Support for ANEURALNETWORKS_PAD and FP16HIDL 1.1 Android 1029Device assignment; AHardwareBuffer supportHIDL 1.2 Android 1130Signed INT8; Control flow; QoS; Fenced executionHIDL 1.3 Android 1231Transition to AIDL; modular system updatesAIDL Android 1535Official DeprecationN/A The structural rigidity of being part of the Android OS meant that new operations and optimizations could only be deployed through annual platform updates. In a field where the state-of-the-art changes quarterly, this annual cadence created a significant "innovation gap". Developers were often forced to target the lowest common denominator, limiting their ability to leverage the latest hardware features.The Drive Toward DeprecationThe official rationale for the deprecation of NNAPI in Android 15 centers on the need for tools that can be updated more frequently than the platform itself. The rise of large language models (LLMs) and diffusion models requires specialized kernels and memory management techniques that were not anticipated in the early design of NNAPI. Furthermore, performance fragmentation remained a persistent issue. While NNAPI intended to provide consistent performance, vendor-specific driver support was often inconsistent. Some drivers might support specific convolution filter sizes (e.g., $3 \times 3$ or $5 \times 5$) but fall back to the CPU for others (e.g., $7 \times 7$), causing unpredictable latency spikes. Research indicates that NNAPI-based implementations could be up to 7x slower than vendor-specific SDKs due to inefficient driver-level support.LiteRT: A Unified Framework for Modern On-Device AILiteRT, the successor to TensorFlow Lite (TFLite), has been architected to resolve the friction between hardware heterogeneity and developer needs. It is no longer restricted to the TensorFlow ecosystem, supporting models trained in PyTorch, JAX, and Keras through high-performance conversion tools.The CompiledModel API and Modular RuntimeThe introduction of the CompiledModel API in LiteRT 2.x represents a significant departure from the legacy Interpreter-based approach. The CompiledModel API is designed to maximize hardware acceleration by automating accelerator selection and optimizing Input/Output (I/O) buffer handling. This modular architecture allows LiteRT to integrate directly with specialized NPU compilers and runtimes from major silicon providers like Qualcomm and MediaTek, bypassing the generic NNAPI layer.One of the core advancements in LiteRT is the "ML Drift" GPU engine. This next-generation engine provides a leap in efficiency, delivering up to 1.4x faster performance than legacy TFLite GPU delegates. By supporting OpenCL, OpenGL, Metal, and WebGPU, LiteRT ensures high-performance acceleration across Android, iOS, Windows, Linux, and web platforms. On Android specifically, the runtime automatically prioritizes OpenCL for peak performance while maintaining OpenGL as a robust fallback for broader device coverage.The Role of NPU AccelerationWhile GPUs offer ubiquitous acceleration—available on approximately 90% of Android devices—the NPU has emerged as the standard component for complex GenAI experiences. Modern NPUs offer tens of Tera Operations Per Second (TOPS) of dedicated AI compute, far exceeding the sustained performance of mobile GPUs while maintaining superior power efficiency per operation.AcceleratorAvailabilityPerformance CharacteristicsBest Use CaseCPU100%Low latency for small models; high versatilitySimple classifiers; fallback GPU~90%High throughput for parallel tasks; floating-point optimizationImage processing; 32-bit/16-bit float models NPU>80% (recent SoCs)Massive TOPS; highest power efficiency; optimized for quantizationLLMs; Generative AI; Real-time scene understanding The strategic shift toward NPUs is reflected in the development of vendor-specific accelerators within the LiteRT framework, such as the Qualcomm AI Engine Direct (QNN) and MediaTek NeuroPilot.Deep Dive: Vendor-Specific Acceleration EcosystemsThe decentralization of neural acceleration has led to a highly specialized ecosystem where each silicon vendor provides unique tools to bridge the gap between their hardware and the LiteRT runtime.Qualcomm AI Engine Direct (QNN)The LiteRT Qualcomm QNN Accelerator is the result of a collaborative effort to replace the previous TFLite QNN delegate with a more direct path to the Hexagon NPU. The Hexagon architecture is built on a Very Long Instruction Word (VLIW) processor that utilizes simultaneous multithreading (SMT) to manage thread-level parallelism and minimize latency.The QNN Accelerator supports approximately 90 LiteRT operations, enabling full delegation for a majority of canonical machine learning models. On the Snapdragon 8 Elite Gen 5 platform, NPU acceleration achieves up to a 100x speedup over the CPU and a 10x speedup over the GPU. This level of performance is exemplified by the FastVLM research model, which achieves a time-to-first-token (TTFT) of just 0.12 seconds and prefill speeds exceeding 11,000 tokens per second on the Snapdragon NPU.The QNN Accelerator's efficiency is further enhanced by specialized kernels for transformer layers, specifically the attention mechanism. By utilizing high-speed INT16 kernels for activation and attention, LiteRT ensures that performance-critical generative models can run with the precision and speed required for real-time interaction.MediaTek NeuroPilot IntegrationMediaTek’s NeuroPilot suite forms the core of its AI ecosystem, providing a unified API to abstract the differences across its diverse range of System-on-Chip (SoC) variants. The MediaTek AI Processing Unit (APU) features a heterogeneous architecture with big, small, and tiny cores, optimized for both bandwidth and power efficiency.The LiteRT NeuroPilot Accelerator provides a direct integration with the NeuroPilot compiler and runtime. Developers can utilize the Neuron SDK to compile models into proprietary Deep Learning Archive (DLA) binaries, resulting in optimized models with reduced latency and memory usage. Benchmarks indicate that models like Gemma 3 can run up to 12x faster on the MediaTek NPU than on a CPU, and 10x faster than on a GPU.Samsung Exynos NPU and AI StudioSamsung's strategy for NPU acceleration centers on the Exynos AI Studio SDK, which functions as a comprehensive toolchain for converting server-trained models into on-device formats. The SDK takes TFLite or ONNX representations as input and transforms them into an internal intermediate representation (IR) optimized for Exynos development.The toolchain is bifurcated into two primary modules:Exynos AI Studio High Level Toolchain (EHT): Performs advanced graph optimization and quantization at the model level (e.g., converting FP32 to INT8, INT16, or FP16).Exynos AI Studio Low Level Toolchain (ELT): Executes lowering operations specific to individual NPU generations.Samsung provides a rigorous verification process using both simulators (to validate the EHT module using the Signal-to-Noise Ratio metric) and emulators (to replicate NPU hardware for ELT validation). This ensures that as models are optimized and compressed, their accuracy is preserved to the greatest extent possible. While LiteRT direct NPU support for Exynos is currently in development, the framework already supports Mali and AMD GPUs on Exynos chips via OpenCL.Technical Mechanisms of Acceleration: AOT vs. JITA critical challenge in mobile ML deployment is the balance between binary portability and the cost of model initialization. LiteRT addresses this through two distinct compilation strategies.Ahead-of-Time (AOT) CompilationAOT compilation is highly recommended for large, complex models where the target SoC is known in advance. By pre-compiling the .tflite model into vendor-specific binaries, developers can drastically reduce first-run initialization costs and peak memory consumption at launch. This strategy provides an "instant-start" experience for the user. For example, ResNet-152 model initialization on an NPU can be reduced from nearly 7.5 seconds (JIT) to under 200 milliseconds through AOT and caching.Just-in-Time (JIT) / On-Device CompilationOnline or on-device compilation is ideal for platform-agnostic distribution, particularly for smaller models. The model is compiled during the application's initialization on the user's device. While this incurs a higher first-run cost, it simplifies the distribution pipeline as a single .tflite file can target a wide range of devices. To mitigate the latency of repeated compilations, LiteRT supports on-device compilation caching, which only triggers re-compilation if the NPU compiler version, build fingerprint, or model itself changes.Memory Optimization and Zero-Copy ExecutionThe performance of mobile ML is often limited not by computational capacity, but by memory bandwidth and the overhead of data transfer.The Roofline Model and Bandwidth SaturationThe "roofline model" is an essential tool for understanding the efficiency of ML accelerators, visualizing where execution falls within compute and memory bounds. In the context of mobile SoCs, a single processing unit (e.g., the GPU alone) often fails to fully saturate the system's memory bandwidth, utilizing only 40-45 GB/s of a theoretical 68 GB/s. However, employing multiple processing units concurrently—such as the GPU and NPU together—can achieve a memory bandwidth utilization closer to 60 GB/s.Zero-Copy Buffers and Memory DomainsTo minimize the energy and latency costs of data movement, LiteRT leverages memory domains and AHardwareBuffer interoperability. By using shared memory buffers, the runtime can transfer data to drivers without the need for the host processor to copy the data. This is particularly critical for real-time video applications that process successive frames from a camera. Native hardware buffer interoperability allows zero-copy data passing directly from OpenGL or OpenCL buffers to the NPU, a fundamental requirement for high-throughput ML pipelines.Generative AI: Challenges and LLM OrchestrationThe integration of Large Language Models (LLMs) into mobile environments introduces unique architectural challenges, specifically the memory-bound nature of the decoding stage.LiteRT-LM and Stateful OrchestrationLiteRT-LM is a specialized orchestration layer built on top of LiteRT to manage the complexities of LLMs, such as KV-cache management and attention mechanism optimization. It powers Gemini Nano deployment across Google products, including the Pixel 6 and Pixel Watch.Benchmarks show that LiteRT demonstrates a significant performance advantage for LLMs, outperforming traditional runtimes like llama.cpp by 3x on the CPU and 7x on the GPU for the decoding phase. When utilizing NPU acceleration for the prefill stage—which is compute-intensive—an additional 2x performance gain is achieved over the GPU.Caching and Speculative DecodingTo accelerate LLM inference, techniques like speculative decoding and advanced caching are employed. Speculative decoding uses a smaller "draft" model to predict tokens that are then verified by the larger "target" model in parallel. Retrieval-based speculative decoding (R-SD) is particularly suited for mobile devices, as it achieves draft latencies under 10ms by using contextual information rather than auxiliary models.Furthermore, for diffusion-based transformers (DiT), a method called "$\Delta$-Cache" has been proposed. Unlike traditional caching methods that reuse entire feature maps—often leading to information loss in transformer architectures—$\Delta$-Cache stores only the change between blocks. This method allows for significant speedups in image and video generation without sacrificing visual quality.Benchmarking Standards: MLPerf and BeyondThe fragmentation of the Android ecosystem necessitates standardized benchmarks to provide objective, reproducible evaluations of ML performance.MLPerf Mobile InferenceThe MLPerf Inference: Mobile benchmark suite, managed by MLCommons, measures how fast mobile devices can produce results using trained models in representative scenarios. The benchmark tasks are designed to be challenging and resistant to domain-specific optimizations, ensuring that results reflect real-world user experiences.MLPerf RoundBenchmark TaskSignificancev5.0Llama 2 70BTransition toward Large GenAI as the industry standard v5.0Llama 3.1 405BNew bar for the scale of generative inference benchmarks v5.1Automotive PointPainting3D object detection for autonomous edge computing MLPerf results indicate that latency has improved by 2x on average between benchmark versions, highlighting the rapid pace of hardware and software co-optimization. However, the benchmarks also underscore the "heterogeneity gap," where vendor-specific SDKs can outperform generic frameworks like NNAPI by significant margins.LiteRT Evaluation ToolingFor practical development, LiteRT provides an extensive set of performance and accuracy-evaluation tools. The LiteRT benchmark tool allows developers to estimate latency and memory usage while supporting multiple flags to test different delegate configurations (e.g., --gpu_backend=gl or --use_nnapi=true for legacy testing). Accuracy evaluation tools—both task-based (e.g., COCO object detection) and task-agnostic (e.g., inference difference comparisons)—help developers justify the minor accuracy tradeoffs often associated with hardware acceleration.Deployment Logistics: Google Play for On-Device AI (PODAI)The transition away from platform-bound APIs has led to a more modular deployment model on Android, leveraging Google Play for On-Device AI (PODAI).AI Packs and Feature DeliveryUnder the PODAI model, developers use LiteRT to export model assets and runtime libraries into an "AI Pack". When a user installs the application, Google Play analyzes the device's hardware and automatically delivers the correct pre-compiled model and vendor-specific runtime libraries. This system abstracts away the complexity of hardware fragmentation while minimizing the binary footprint of the application.Library Conflict ManagementA notable challenge in this transition is the conflict between standalone LiteRT libraries and the Play Services runtime. Apps that attempt to use both simultaneously may face build failures due to duplicated class errors, as Play Services libraries often still depend on legacy TFLite API packages. Developers must explicitly exclude these legacy dependencies to force the build to use the modern LiteRT artifacts.The Impact of Android 15 Platform ChangesBeyond the deprecation of NNAPI, Android 15 (SDK 35) introduces several behavioral changes that impact how ML applications operate.Foreground Service RestrictionsAndroid 15 imposes stricter limits on foreground services, which are often used for background ML processing or data synchronization. A new onTimeout(int, int) callback is introduced for dataSync and mediaProcessing services, with a timeout of 6 hours per 24-hour period. This requires ML developers to design more robust, intermittent processing pipelines rather than relying on continuous execution.SDK Migration and Gradle CompatibilityMigrating to the Android 15 SDK requires updates to Gradle (version 8.0 or higher) and careful management of Java/Kotlin API collisions. Older versions of the Android Asset Packaging Tool (AAPT2) may crash when handling the newer binary structures used in API 35, highlighting the need for a holistic update of the development toolchain during the transition to LiteRT.Future Outlook: Toward Heterogeneous ParallelismThe future of mobile ML inference lies in the deeper integration of heterogeneous processors. Frameworks like "HeteroInfer" are being developed to fully exploit both GPU and NPU computational power concurrently. By implementing tensor-level GPU-NPU parallelism—where compute graphs are partitioned based on tensor characteristics—researchers have achieved speedups ranging from 1.34x to 6.02x compared to NPU-only or GPU-only frameworks.This approach addresses the "NPU underutilization" often seen when specific tensor shapes do not align perfectly with the NPU's hardware architecture. As mobile SoCs continue to evolve with unified memory architectures (UMA), the ability to synchronize execution across different accelerators in a single address space will become the defining feature of high-performance mobile AI.The move from NNAPI to LiteRT is more than a technical migration; it is a strategic repositioning of the Android ecosystem to thrive in an era where AI models are the primary drivers of hardware innovation. By decoupling the ML runtime from the OS and providing direct, high-performance paths to NPU silicon, Google and its hardware partners have created a sustainable foundation for the next decade of on-device intelligence.